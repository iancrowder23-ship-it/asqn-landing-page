---
phase: 08-vps-provisioning-and-production-compose
plan: 03
type: execute
wave: 2
depends_on: ["08-01", "08-02"]
files_modified:
  - src/routes/health/+server.ts
  - docker-compose.yml
  - docker-compose.dev.yml
  - Caddyfile
autonomous: false

must_haves:
  truths:
    - "GET /health returns HTTP 200 with body 'ok'"
    - "docker-compose.yml defines app + caddy services on a shared internal network"
    - "App container uses expose (not ports) — not directly reachable from internet"
    - "Caddy reverse proxies asqnmilsim.us to app:3000 with staging ACME TLS"
    - "Both services have restart: unless-stopped policy"
    - "caddy_data named volume persists TLS certificates across restarts"
    - "ORIGIN is set to https://asqnmilsim.us in the production environment"
    - "Production .env on VPS contains Supabase keys and is not in git or Docker image"
    - "Visiting https://asqnmilsim.us returns the application (staging cert warning expected)"
  artifacts:
    - path: "src/routes/health/+server.ts"
      provides: "Health check endpoint"
      exports: ["GET"]
    - path: "docker-compose.yml"
      provides: "Production Compose stack with app + Caddy"
      contains: "caddy_data"
    - path: "docker-compose.dev.yml"
      provides: "Dev Compose preserved for local development"
      contains: "ports.*3000"
    - path: "Caddyfile"
      provides: "Caddy reverse proxy config with staging ACME"
      contains: "acme_ca.*staging"
  key_links:
    - from: "Caddyfile"
      to: "app:3000"
      via: "reverse_proxy directive"
      pattern: "reverse_proxy app:3000"
    - from: "docker-compose.yml caddy service"
      to: "Caddyfile"
      via: "volume mount"
      pattern: "Caddyfile:/etc/caddy/Caddyfile"
    - from: "docker-compose.yml app service"
      to: ".env"
      via: "env_file directive"
      pattern: "env_file.*\\.env"
    - from: "src/routes/health/+server.ts"
      to: "HTTP 200"
      via: "SvelteKit API route GET handler"
      pattern: "new Response.*ok.*200"
---

<objective>
Create the production Docker Compose stack: add a /health SvelteKit endpoint, replace the dev docker-compose.yml with a production-grade configuration (app + Caddy reverse proxy on internal network), create the Caddyfile with staging ACME, and deploy the stack on the VPS.

Purpose: This brings the application online at https://asqnmilsim.us with auto-HTTPS (staging certificates for now) and ensures the app is only accessible through the Caddy reverse proxy, not directly on port 3000.
Output: Application running at https://asqnmilsim.us with staging TLS certificate, health endpoint responding, app not directly reachable from internet.
</objective>

<execution_context>
@/home/iancrowder/.claude/get-shit-done/workflows/execute-plan.md
@/home/iancrowder/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-vps-provisioning-and-production-compose/08-RESEARCH.md
@.planning/phases/08-vps-provisioning-and-production-compose/08-01-SUMMARY.md
@.planning/phases/08-vps-provisioning-and-production-compose/08-02-SUMMARY.md
</context>

<important_note>
ALL npm/npx commands need `env -u NPM_CONFIG_PREFIX` prefix due to shell issue.

This plan has code changes (Tasks 1-2, autonomous) and VPS deployment (Task 3, requires user SSH to VPS). The code changes must be committed and pushed BEFORE the VPS deployment step.

Use staging ACME CA throughout this phase per project notes. Browser will show certificate warning — this is expected and correct. Phase 10 switches to production ACME.
</important_note>

<tasks>

<task type="auto">
  <name>Task 1: Create /health endpoint and production Compose files</name>
  <files>
    src/routes/health/+server.ts
    docker-compose.yml
    docker-compose.dev.yml
    Caddyfile
  </files>
  <action>
**Step 1: Create the /health endpoint**

Create `src/routes/health/+server.ts`:
```typescript
import type { RequestHandler } from './$types';

export const GET: RequestHandler = () => {
    return new Response('ok', { status: 200 });
};
```

This is a simple SvelteKit API route. No auth required — health checks must be unauthenticated.

**Step 2: Preserve existing dev Compose as docker-compose.dev.yml**

Copy the current `docker-compose.yml` to `docker-compose.dev.yml` so local development still works:
```bash
cp docker-compose.yml docker-compose.dev.yml
```

The dev compose file stays as-is (ports: "3000:3000", ORIGIN=http://localhost:3000).

**Step 3: Replace docker-compose.yml with production stack**

Overwrite `docker-compose.yml` with the production configuration:
```yaml
services:
  app:
    build: .
    expose:
      - "3000"
    env_file: .env
    environment:
      - ORIGIN=https://asqnmilsim.us
      - NODE_ENV=production
    restart: unless-stopped
    networks:
      - internal

  caddy:
    image: caddy:2-alpine
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy_config:/config
    networks:
      - internal
    depends_on:
      - app

networks:
  internal:

volumes:
  caddy_data:
  caddy_config:
```

Key differences from dev compose:
- `expose` instead of `ports` on app (COMPOSE-03: internal only)
- `restart: unless-stopped` on both services (COMPOSE-04)
- `ORIGIN=https://asqnmilsim.us` (COMPOSE-05)
- Caddy service with ports 80/443 (COMPOSE-01, COMPOSE-02)
- `caddy_data` named volume (COMPOSE-06)
- Internal Docker network shared between app and caddy

**Step 4: Create the Caddyfile**

Create `Caddyfile` in the project root:
```
{
    acme_ca https://acme-staging-v02.api.letsencrypt.org/directory
}

asqnmilsim.us {
    reverse_proxy app:3000
}
```

The staging ACME CA is intentional — it avoids hitting Let's Encrypt production rate limits during development. Phase 10 will remove this line and switch to production certs.

**Step 5: Verify the build works locally**

```bash
env -u NPM_CONFIG_PREFIX npm run build
```

Should succeed with no errors (the /health route is just a simple GET handler).

**Step 6: Add Caddyfile and docker-compose.dev.yml to .dockerignore**

Edit `.dockerignore` to add:
```
Caddyfile
docker-compose*.yml
```

These files should not be in the Docker image — they are mounted or used by Compose externally.

**Step 7: Commit and push**

Stage and commit the new/modified files:
```bash
git add src/routes/health/+server.ts docker-compose.yml docker-compose.dev.yml Caddyfile .dockerignore
git commit -m "feat(08-03): add health endpoint, production compose with Caddy"
git push origin main
```
  </action>
  <verify>
```bash
# Build succeeds
env -u NPM_CONFIG_PREFIX npm run build

# Health endpoint file exists
cat src/routes/health/+server.ts

# Production compose has correct structure
grep -q "expose" docker-compose.yml && echo "OK: expose not ports"
grep -q "caddy_data" docker-compose.yml && echo "OK: caddy volume"
grep -q "unless-stopped" docker-compose.yml && echo "OK: restart policy"

# Caddyfile has staging ACME
grep -q "acme-staging" Caddyfile && echo "OK: staging ACME"

# Dev compose preserved
cat docker-compose.dev.yml | grep "3000:3000" && echo "OK: dev compose preserved"

# Changes pushed
git log origin/main --oneline -1
```
  </verify>
  <done>
/health endpoint exists. Production docker-compose.yml has app + Caddy on internal network with restart policies and named volumes. Dev compose preserved as docker-compose.dev.yml. Caddyfile uses staging ACME. All changes committed and pushed to GitHub.
  </done>
</task>

<task type="checkpoint:human-action" gate="blocking">
  <name>Task 2: Deploy production stack on VPS</name>
  <action>
The user must SSH into the VPS as the deploy user and run these commands. Present them clearly.

**Prerequisites:** DNS must be resolving (from 08-01). Verify first:
```bash
dig asqnmilsim.us +short
# Must return VPS_IP. If not, Caddy TLS will fail — wait for propagation.
```

**Step A: Clone the repository on VPS**

SSH to VPS as deploy user:
```bash
ssh -i ~/.ssh/asqn_deploy deploy@VPS_IP
```

Clone into /opt/asqn:
```bash
cd /opt/asqn
git clone https://github.com/USERNAME/asqn-landing-page.git .
# OR if private repo requires SSH:
git clone git@github.com:USERNAME/asqn-landing-page.git .
```

Note: If the repo is private and HTTPS clone fails, the user may need to set up a GitHub deploy key or use a personal access token. SSH clone (`git@github.com:...`) requires the deploy user to have a GitHub SSH key configured.

Alternative if git clone is difficult: From the local machine, use `scp` to copy the necessary files:
```bash
scp -i ~/.ssh/asqn_deploy docker-compose.yml Caddyfile deploy@VPS_IP:/opt/asqn/
scp -i ~/.ssh/asqn_deploy -r . deploy@VPS_IP:/opt/asqn/
```

**Step B: Create the production .env file**

On the VPS in /opt/asqn, create `.env` with the production values:
```bash
cat > /opt/asqn/.env << 'EOF'
PUBLIC_SUPABASE_URL=https://lelwuinxszfwnlquwsho.supabase.co
PUBLIC_SUPABASE_PUBLISHABLE_KEY=<anon key from Supabase dashboard>
SUPABASE_SERVICE_ROLE_KEY=<service role key from Supabase dashboard>
ORIGIN=https://asqnmilsim.us
NODE_ENV=production
EOF
```

The user must fill in the actual Supabase keys. These can be found in the Supabase Dashboard under Project Settings > API.

Secure the file:
```bash
chmod 600 /opt/asqn/.env
```

**Step C: Build and start the production stack**

```bash
cd /opt/asqn
docker compose up --build -d
```

This will:
1. Build the app image from the Dockerfile (multi-stage, may take 2-5 minutes on first run)
2. Pull the caddy:2-alpine image
3. Start both containers on the internal network
4. Caddy will attempt ACME challenge for asqnmilsim.us (staging cert)

**Step D: Verify containers are running**

```bash
docker compose ps
# Both 'app' and 'caddy' should show status "Up"

docker compose logs caddy --tail 20
# Look for "certificate obtained successfully" or similar ACME success

docker compose logs app --tail 10
# Look for "Listening on 0.0.0.0:3000" or similar startup message
```

**Step E: Test from external machine (local terminal)**

```bash
# Test HTTPS (staging cert — use -k to accept self-signed)
curl -k https://asqnmilsim.us
# Should return HTML content

# Test health endpoint
curl -k https://asqnmilsim.us/health
# Should return "ok" with HTTP 200

# Test that port 3000 is NOT directly accessible
curl http://VPS_IP:3000
# Should return "Connection refused"
```

**Step F: Verify restart persistence**

On VPS:
```bash
# Restart Docker daemon
sudo systemctl restart docker

# Wait 10 seconds for containers to restart
sleep 10

# Check containers came back
docker compose ps
# Both should show "Up" status
```
  </action>
  <verify>
All verification from external machine (local terminal):
```bash
# HTTPS works (staging cert)
curl -kI https://asqnmilsim.us
# Expected: HTTP/2 200

# Health endpoint responds
curl -k https://asqnmilsim.us/health
# Expected: ok

# Port 3000 not reachable
curl http://VPS_IP:3000 2>&1 | grep -i "refused"
# Expected: Connection refused

# From VPS — containers running
ssh -i ~/.ssh/asqn_deploy deploy@VPS_IP "docker compose -f /opt/asqn/docker-compose.yml ps"
# Expected: both services Up
```
  </verify>
  <done>
Production stack running on VPS. https://asqnmilsim.us serves the application with staging TLS certificate. /health returns 200. Port 3000 is not directly accessible from internet. Containers restart after Docker daemon restart. Production .env contains Supabase keys and is not in git.
  </done>
</task>

</tasks>

<verification>
Phase 8 Success Criteria verification:
1. `curl -k https://asqnmilsim.us` — returns application HTML over HTTPS (staging cert warning expected)
2. `curl http://VPS_IP:3000` — returns connection refused (app not directly reachable)
3. `sudo systemctl restart docker && sleep 15 && docker compose ps` — both containers Up (auto-restart)
4. `curl -k https://asqnmilsim.us/health` — returns HTTP 200 OK with body "ok"
5. `.env` is on VPS at `/opt/asqn/.env` with Supabase keys, NOT in git repo or Docker image
</verification>

<success_criteria>
- COMPOSE-01: docker-compose.yml runs app + Caddy on shared internal Docker network
- COMPOSE-02: Caddy reverse proxies asqnmilsim.us with auto-provisioned HTTPS (staging ACME)
- COMPOSE-03: App container uses expose (not ports) — not directly reachable from internet
- COMPOSE-04: Both services have restart: unless-stopped policy
- COMPOSE-05: ORIGIN=https://asqnmilsim.us set in production environment
- COMPOSE-06: caddy_data named volume persists TLS certificates across restarts
- COMPOSE-07: /health endpoint returns HTTP 200 OK
- SEC-02: Production .env on VPS contains Supabase keys, never in git or Docker image
</success_criteria>

<output>
After completion, create `.planning/phases/08-vps-provisioning-and-production-compose/08-03-SUMMARY.md`
</output>
